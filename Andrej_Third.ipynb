{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading in all of words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s : i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "print(stoi)\n",
    "itos = {i : s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "block_size = 3 #content length, from how much do we want to predict next char\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context = [0] * 3 #outputs [0,0,0]\n",
    "    for chr in w + '.':\n",
    "        ix = stoi[chr]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context) + ' ----> ' + itos[ix])\n",
    "        context = context[1:] + [ix] #creating new list with new char appended to finish\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182308, 3]) torch.Size([22886, 3]) torch.Size([22952, 3])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(iwords):\n",
    "    X, Y = [], []\n",
    "    block_size = 3 #content length, from how much do we want to predict next char\n",
    "    for w in iwords:\n",
    "        # print(w)\n",
    "        context = [0] * 3 #outputs [0,0,0]\n",
    "        for chr in w + '.':\n",
    "            ix = stoi[chr]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context) + ' ----> ' + itos[ix])\n",
    "            context = context[1:] + [ix] #creating new list with new char appended to finish\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "import random\n",
    "random.seed(1)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "print(Xtr.shape, Xval.shape, Xte.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182308, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "C = torch.randn(27, 2) #we have 27 different characters and for now, we want 2 dimension word embedding\n",
    "emb = C[Xtr] #getting embeddings for all indexes we created from context 2d array \n",
    "#basically it looks up all of integer values X is stored and places their respective 2 value arrays into them\n",
    "print(emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(6, 100) #100 neurons are choice, getting 6 is total vectors we want to have.\n",
    "b1 = torch.randn(100) \n",
    "#we need to convert embed shape of torch.Size([32, 3, 2]) to torch.Size([32, 6])\n",
    "#you can do: \n",
    "#torch.cat(emb[:, 0, :], emb[:, 1, :], emb[:, 2, :], 1) // creates a whole new tensor in memory\n",
    "#torch.cat(torch.unbind(emb, 1), 1)\n",
    "#but most efficient way is without creating any additional memory in computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182308, 100])\n"
     ]
    }
   ],
   "source": [
    "#emb = emb.view(emb.shape[0], emb.shape[1] * emb.shape[2]) // instead of doing this, do below, torch will infer shape number automaticlly\n",
    "emb = emb.view(-1, 6) #just write 6 but in prod, you want to do  emb.shape[1] * emb.shape[2]\n",
    "h = torch.tanh(emb @ W1 + b1)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn(100, 27)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -prob[torch.arange(prob.shape[0]), Ytr].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###================wrapping everything at once========================###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn(27,10) #before 2 now 10\n",
    "W1 = torch.randn(30, 200) # before 100 now 300, before 300 now 200 since we want to know spesificly if embedding increase will affect result\n",
    "B1 = torch.randn(200)  # before 100 now 300 before 300 now 200\n",
    "W2 = torch.randn(200, 27)  # before 100 now 300 before 300 now 200\n",
    "B2 = torch.randn(27)\n",
    "params = [C, W1, B1, W2, B2]\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(n.nelement() for n in params) # total changable weights in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lre = torch.linspace(0.001, 1, 100)\n",
    "# print(lre) # this is bad approach since it is completely linear\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10 ** lre # this is way to much good practice since values are exponentially decreasing and then deexponentially decreasing\n",
    "##IMPORTANT the way we decided on 0,001 and 1 learning rate is actually by looking our spesifics of our data and see where it ACTUALLY EXPLODES, where does grads actually becomes absurd, \n",
    "#we need to decide on threshold values from looking our grad datas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss for all of the results validation; loss.item() = 2.2811198234558105\n"
     ]
    }
   ],
   "source": [
    "lres = []\n",
    "losses = []\n",
    "\n",
    "for i in range(30000):\n",
    "    #minibatch \n",
    "    ix = torch.randint(0, Xtr.shape[0], (32, )) # 32 sizes of batches \n",
    "\n",
    "    #forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1 + B1) # 32, 100\n",
    "    logits = h @ W2 + B2 # 32 27\n",
    "    # counts = logits.exp()\n",
    "    # probs = counts / counts.sum(1, keepdim=True)\n",
    "    # print(probs.shape)\n",
    "    # loss = -probs[torch.arange(probs.shape[0]), Y].log().mean()\n",
    "    # 1. the same exact result appears when we use cross entropy func becuase it does not create any additional memory for all intermediate steps\n",
    "    # 2. backward pass would be efficent because instead of going through derivatives or every operation of softmax // log, -minus division etc., it will already have predefined backward func.\n",
    "    # 3. last reason is when there is too large values for logits, say, 100, exp operasion outputs inf, to overcome this, it finds maximum number of array of logits and subtract each element from it \n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # print(f'{loss.item() = }')\n",
    "\n",
    "    #backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # lr = lrs[i]\n",
    "    for p in params:\n",
    "        # p.data += -lr * p.grad\n",
    "        p.data += -0.01 * p.grad\n",
    "\n",
    "    #graphin learning rate decay\n",
    "    # lres.append(lre[i])\n",
    "    # losses.append(loss.item())\n",
    "\n",
    "\n",
    "#right now this modal 'overfitting' the 32 examples we give to data very easily, we cannot get 0 because\n",
    "#there are for example ... ---> e ... ---> t multiple contexts pointing different variables, so we should not be getting 0\n",
    "\n",
    "#the losses for the batches only calculate loss according to that batch so below calculating whole loss\n",
    "emb = C[Xte] \n",
    "h = torch.tanh(emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1 + B1) \n",
    "logits = h @ W2 + B2 \n",
    "loss = F.cross_entropy(logits, Yte)\n",
    "print(f'final loss for all of the results validation; {loss.item() = }')\n",
    "\n",
    "#### when we also evaluate overall loss in training and loss values are near to each other, we say we dont overfit data\n",
    "#### but we underfit the data because model is not powerful enught with its small param num to memorize whole set\n",
    "#### to avoid underfit, we increase params count. RESULT: didnt much improved loss, still getting underfit but overall loss decreased at most to 2.257 before.. 2.3 ishhh /// our model is slightly better than random guess === ln(27) == 3.3\n",
    "#### increasing word embedding dim from 2 to 10, RESULT: loss decreased so this means its problem of embedding 2.257 to 2.137 \n",
    "\n",
    "\n",
    "# emb = C[Xtr] \n",
    "# h = torch.tanh(emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1 + B1) \n",
    "# logits = h @ W2 + B2 \n",
    "# loss = F.cross_entropy(logits, Ytr)\n",
    "# print(f'final loss for all of the results training; {loss.item() = }')\n",
    "\n",
    "#### the result we see in this graph is, we need to use 0.1 ish result for our learning rate, the minimum of the grapgh, its the place where loss is\n",
    "# is much more stable \n",
    "# plt.plot(lres, losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### as we add more and more parameters to our model, we are overfitting training data, we can near 0 loss with hundreds of millions of neurons\n",
    "### but when we try to sample from data, it will give exact same result we get from training data.\n",
    "### so what we do to evaluate these results is split data into, training, dev/validation, test splits /// 80% 10% 10\n",
    "\n",
    "#normally you dont wait for nn training to finish like this, you assign bunch of jobs to cpu and wait them finish in days,\n",
    "#normally you would initilieeze all of hyperparameteres as a single variables and change them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "Training set (Xtr):   torch.Size([182254, 3]) (182254 examples, 3 context length)\n",
      "Validation set (Xval): torch.Size([22914, 3]) (22914 examples, 3 context length)\n",
      "Test set (Xte):      torch.Size([22978, 3]) (22978 examples, 3 context length)\n",
      "Total examples: 228146\n",
      "Model Parameter Count Breakdown:\n",
      "C  (char embeddings): 270 parameters (torch.Size([27, 10]): 27 chars × 10 embedding dim)\n",
      "W1 (first weight):    6,000 parameters (torch.Size([30, 200]): 30 input × 200 hidden)\n",
      "B1 (first bias):      200 parameters (torch.Size([200]): 200 hidden)\n",
      "W2 (second weight):   5,400 parameters (torch.Size([200, 27]): 200 hidden × 27 output)\n",
      "B2 (second bias):     27 parameters (torch.Size([27]): 27 output)\n",
      "--------------------------------\n",
      "Total Parameters:     11,897\n",
      "Model Performance:\n",
      "Training Loss:   2.2964\n",
      "Validation Loss: 2.3041\n",
      "Random Guess:    3.2958 (ln(27) - theoretical worst case)\n",
      "Gap to Random:   0.9918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######========================== AAAALLL of them wrapped in single cell.==================================#####\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s : i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i : s for s, i in stoi.items()}\n",
    "\n",
    "def build_dataset(iwords):  \n",
    "    X, Y = [], []\n",
    "    block_size = 3 #content length, from how much do we want to predict next char\n",
    "    for w in iwords:\n",
    "        # print(w)\n",
    "        context = [0] * 3 #outputs [0,0,0]\n",
    "        for chr in w + '.':\n",
    "            ix = stoi[chr]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context) + ' ----> ' + itos[ix])\n",
    "            context = context[1:] + [ix] #creating new list with new char appended to finish\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "print(f\"\"\"Dataset splits:\n",
    "Training set (Xtr):   {Xtr.shape} ({Xtr.shape[0]} examples, {Xtr.shape[1]} context length)\n",
    "Validation set (Xval): {Xval.shape} ({Xval.shape[0]} examples, {Xval.shape[1]} context length)\n",
    "Test set (Xte):      {Xte.shape} ({Xte.shape[0]} examples, {Xte.shape[1]} context length)\n",
    "Total examples: {Xtr.shape[0] + Xval.shape[0] + Xte.shape[0]}\"\"\")\n",
    "\n",
    "##initilize parameters\n",
    "C = torch.randn(27,10) \n",
    "W1 = torch.randn(30, 200) \n",
    "B1 = torch.randn(200)  \n",
    "W2 = torch.randn(200, 27)  \n",
    "B2 = torch.randn(27)\n",
    "params = [C, W1, B1, W2, B2]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "total_params = sum(n.nelement() for n in params)\n",
    "print(f\"\"\"Model Parameter Count Breakdown:\n",
    "C  (char embeddings): {C.nelement():,} parameters ({C.shape}: 27 chars × 10 embedding dim)\n",
    "W1 (first weight):    {W1.nelement():,} parameters ({W1.shape}: 30 input × 200 hidden)\n",
    "B1 (first bias):      {B1.nelement():,} parameters ({B1.shape}: 200 hidden)\n",
    "W2 (second weight):   {W2.nelement():,} parameters ({W2.shape}: 200 hidden × 27 output)\n",
    "B2 (second bias):     {B2.nelement():,} parameters ({B2.shape}: 27 output)\n",
    "--------------------------------\n",
    "Total Parameters:     {total_params:,}\"\"\")\n",
    "\n",
    "for i in range(100000):\n",
    "    #minibatch \n",
    "    ix = torch.randint(0, Xtr.shape[0], (32, ))  \n",
    "\n",
    "    #forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1 + B1) # 32, 100\n",
    "    logits = h @ W2 + B2 # 32 27\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "\n",
    "    #backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i < 50000 else 0.01\n",
    "    for p in params:\n",
    "        p.data += -0.01 * p.grad\n",
    "\n",
    "#val loss\n",
    "emb = C[Xval] \n",
    "h = torch.tanh(emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1 + B1) \n",
    "logits = h @ W2 + B2 \n",
    "val_loss = F.cross_entropy(logits, Yval)\n",
    "\n",
    "#train loss\n",
    "emb = C[Xtr] \n",
    "h = torch.tanh(emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1 + B1) \n",
    "logits = h @ W2 + B2 \n",
    "train_loss = F.cross_entropy(logits, Ytr)\n",
    "\n",
    "print(f\"\"\"Model Performance:\n",
    "Training Loss:   {train_loss.item():.4f}\n",
    "Validation Loss: {val_loss.item():.4f}\n",
    "Random Guess:    {torch.log(torch.tensor(27.0)):.4f} (ln(27) - theoretical worst case)\n",
    "Gap to Random:   {torch.log(torch.tensor(27.0)).item() - val_loss.item():.4f}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance on Test Set:\n",
      "Test Loss:      2.3227\n",
      "Random Guess:   3.2958 (ln(27) - theoretical worst case)\n",
      "Gap to Random:  0.9731\n",
      "\n",
      "Performance Analysis:\n",
      "- The model achieves a test loss of 2.3227, which is \n",
      "  0.9731 better than random guessing\n",
      "- For reference, random guessing would give a loss of ln(27) ≈ 3.2958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate test set loss\n",
    "emb = C[Xte] \n",
    "h = torch.tanh(emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1 + B1) \n",
    "logits = h @ W2 + B2 \n",
    "test_loss = F.cross_entropy(logits, Yte)\n",
    "\n",
    "print(f\"\"\"Model Performance on Test Set:\n",
    "Test Loss:      {test_loss.item():.4f}\n",
    "Random Guess:   {torch.log(torch.tensor(27.0)):.4f} (ln(27) - theoretical worst case)\n",
    "Gap to Random:  {torch.log(torch.tensor(27.0)).item() - test_loss.item():.4f}\n",
    "\n",
    "Performance Analysis:\n",
    "- The model achieves a test loss of {test_loss.item():.4f}, which is \n",
    "  {(torch.log(torch.tensor(27.0)).item() - test_loss.item()):.4f} better than random guessing\n",
    "- For reference, random guessing would give a loss of ln(27) ≈ {torch.log(torch.tensor(27.0)):.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalishah.\n",
      "bryen.\n",
      "eviovaniah.\n",
      "elin.\n",
      "devon.\n",
      "laurarle.\n",
      "harisah.\n",
      "suham.\n",
      "bryva.\n",
      "bavoy.\n",
      "hanis.\n",
      "ize.\n",
      "ost.\n",
      "cara.\n",
      "kiock.\n",
      "set.\n",
      "kehania.\n",
      "xavtenm.\n",
      "satela.\n",
      "alexia.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * 3 # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + B1)\n",
    "      logits = h @ W2 + B2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
