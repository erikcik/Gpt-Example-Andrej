{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('input.txt', 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have vocab size of: 65\n",
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "vocab_size = sorted(list(set(text)))\n",
    "vocab_length = len(vocab_size)\n",
    "print(f'We have vocab size of: {len(vocab_size)}')\n",
    "stoi = {s : i for i, s in enumerate(vocab_size)}\n",
    "itos = {i : s for s, i in stoi.items()}\n",
    "encoder = lambda f: [stoi[i] for i in f]\n",
    "decoder = lambda f: ''.join([itos[i] for i in f])\n",
    "x = encoder('hello')\n",
    "print(x)\n",
    "y = decoder(x)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context=tensor([18])==target=tensor(47)\n",
      "context=tensor([18, 47])==target=tensor(56)\n",
      "context=tensor([18, 47, 56])==target=tensor(57)\n",
      "context=tensor([18, 47, 56, 57])==target=tensor(58)\n",
      "context=tensor([18, 47, 56, 57, 58])==target=tensor(1)\n",
      "context=tensor([18, 47, 56, 57, 58,  1])==target=tensor(15)\n",
      "context=tensor([18, 47, 56, 57, 58,  1, 15])==target=tensor(47)\n",
      "context=tensor([18, 47, 56, 57, 58,  1, 15, 47])==target=tensor(58)\n"
     ]
    }
   ],
   "source": [
    "#splitting dataset to training and validation\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] \n",
    "block_size = 8\n",
    "\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "###small overview of our batches in small scale\n",
    "for i in range(block_size):\n",
    "    context = x[:i + 1]\n",
    "    target = y[i]\n",
    "    print(f'{context=}=={target=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[47, 57,  1, 52, 39, 51, 43, 10],\n",
      "        [ 1, 49, 52, 53, 61,  1, 58, 46],\n",
      "        [49, 57,  8,  0,  0, 23, 21, 26],\n",
      "        [56, 43,  2,  0,  0, 13, 26, 19]])\n",
      "outputs: tensor([[57,  1, 52, 39, 51, 43, 10,  1],\n",
      "        [49, 52, 53, 61,  1, 58, 46, 43],\n",
      "        [57,  8,  0,  0, 23, 21, 26, 19],\n",
      "        [43,  2,  0,  0, 13, 26, 19, 17]])\n",
      "when context is: [47] target is: 57\n",
      "when context is: [47, 57] target is: 1\n",
      "when context is: [47, 57, 1] target is: 52\n",
      "when context is: [47, 57, 1, 52] target is: 39\n",
      "when context is: [47, 57, 1, 52, 39] target is: 51\n",
      "when context is: [47, 57, 1, 52, 39, 51] target is: 43\n",
      "when context is: [47, 57, 1, 52, 39, 51, 43] target is: 10\n",
      "when context is: [47, 57, 1, 52, 39, 51, 43, 10] target is: 1\n",
      "when context is: [1] target is: 49\n",
      "when context is: [1, 49] target is: 52\n",
      "when context is: [1, 49, 52] target is: 53\n",
      "when context is: [1, 49, 52, 53] target is: 61\n",
      "when context is: [1, 49, 52, 53, 61] target is: 1\n",
      "when context is: [1, 49, 52, 53, 61, 1] target is: 58\n",
      "when context is: [1, 49, 52, 53, 61, 1, 58] target is: 46\n",
      "when context is: [1, 49, 52, 53, 61, 1, 58, 46] target is: 43\n",
      "when context is: [49] target is: 57\n",
      "when context is: [49, 57] target is: 8\n",
      "when context is: [49, 57, 8] target is: 0\n",
      "when context is: [49, 57, 8, 0] target is: 0\n",
      "when context is: [49, 57, 8, 0, 0] target is: 23\n",
      "when context is: [49, 57, 8, 0, 0, 23] target is: 21\n",
      "when context is: [49, 57, 8, 0, 0, 23, 21] target is: 26\n",
      "when context is: [49, 57, 8, 0, 0, 23, 21, 26] target is: 19\n",
      "when context is: [56] target is: 43\n",
      "when context is: [56, 43] target is: 2\n",
      "when context is: [56, 43, 2] target is: 0\n",
      "when context is: [56, 43, 2, 0] target is: 0\n",
      "when context is: [56, 43, 2, 0, 0] target is: 13\n",
      "when context is: [56, 43, 2, 0, 0, 13] target is: 26\n",
      "when context is: [56, 43, 2, 0, 0, 13, 26] target is: 19\n",
      "when context is: [56, 43, 2, 0, 0, 13, 26, 19] target is: 17\n"
     ]
    }
   ],
   "source": [
    "#creating batches of data to process in block_size\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "def get_batch(label):\n",
    "    data = train_data if label == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #creates 2d tensor from all of the lists of values we provide\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "    \n",
    "xb, xy = get_batch('train')\n",
    "print(f'inputs: {xb}') \n",
    "print(f'outputs: {xy}')\n",
    "\n",
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        context = xb[i, :j + 1]\n",
    "        target = xy[i, j]     \n",
    "        print(f'when context is: {context.tolist()} target is: {target}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "4.820847034454346\n",
      "\n",
      "N3RgbEzo$eSD&FzydNMx.:aCM3MVXjfmFMkXjhyKM$bYeS.b&Ot'KZkDrAYGz3eYIldNILLQWpZFUnO K-KYyTpuPfzxBmqmC!.CbK?uUgMW3SdH;&ku3OPiUiTnUKqb:QYRQWgN:Nx'dupE;ZrOIIbU&KA&AMFrfzWyx?LcjbxXgWb:opN!,ZbE:u3K:oRXzK&wwCa$\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiagramModel(nn.Module):\n",
    "    def __init__(self, xdim, ydim):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(xdim, ydim) #from second lesson, we initiliez one hot representation to all indexes, but now we are doing it for embedding\n",
    "        #the reason x dim and y dim should be vocab size is that for cross entropy loss, the compared values after softmax is targets indexes and logits embedding values.\n",
    "        # and xdim is vocab size because the iterated indexes are within the interval of 0 and vocab size.\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx) #B T C, it will generate batch size, block_size, vocab_size // assigning vocab sized embed to each index\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            # getting last index of array and getting that time dimensions element\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            ## when you dont do dim 1 in here very strangely it closes down cursor, strange. but this line basically does concatting tensors on dim 1\n",
    "            ## because they output as:: [[2]] [[23, 33]]\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BiagramModel(vocab_length, vocab_length)\n",
    "out, loss = m(xb, xy)\n",
    "print(out.shape)\n",
    "print(loss.item())\n",
    "\n",
    "print(decoder(m.generate(torch.zeros((1, 1), dtype=torch.long), 200)[0].tolist()))\n",
    "#there is issue in init loss becuase we expect loss to be -ln(1/65) --> 4.174 but rn we get 4.588 which is we need to normalize init weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.430536985397339\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, xy = get_batch('train')\n",
    "    logits, loss = m(xb, xy)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAloitemariarderecos iend CENCO:\n",
      "II ES: is d?\n",
      "AMul g sweas ingseveven:\n",
      "\n",
      "ANorwin'ss:\n",
      "S:\n",
      "S:\n",
      "ist bur the avigembed If; d t:\n",
      "ngousin fin y'llld h'ste thapert t y nowaind y My hest;\n",
      "Ange:\n",
      "\n",
      "CERueve akiamey \n"
     ]
    }
   ],
   "source": [
    "print(decoder(m.generate(torch.zeros((1, 1), dtype=torch.long), 200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####attention section####\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros(B, T, C) #xbow stands for bag of words, common term when we try to predict context of words based on some calculation over all of its previous chars\n",
    "#we try to get the mean of all of the C channels for its prevous values and storing this info into xbow\n",
    "#this is most basic way to capture meaning from other words, taking mean would make us lose very much info about the words.\n",
    "\n",
    "for i in range(B):\n",
    "    for j in range(T):\n",
    "        x_prev = x[i, :j+1]\n",
    "        xbow[i, j] = x_prev.mean(0)\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doing the same thing as above but with much much more efficient manner\n",
    "torch.manual_seed(3)\n",
    "wei = torch.tril(torch.ones(T,T)) # the reason we do this is that, we dont want future tokens to be able to talk to each other so we just initilize them as zero\n",
    "wei = wei / wei.sum(1, keepdim=True) # getting probability distribution\n",
    "print(wei)\n",
    "wout = wei @ x # (T, T) @ (B, T, C) ==> (B, T, T) @ (B, T, C) ==> (B, T, C)\n",
    "#generating exact same result but with matrix mult. \n",
    "torch.allclose(wout, xbow)#checking if all results are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##creating with softmax, this implementation is much better because with this, wei's values are data-dependent, we just at first initilize at 0\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = torch.zeros((T,T)) #think of it as interaction strength of attention, how \"\"\"\"much\"\"\"\" of the past values do we want to take mean of.\n",
    "#some tokens will find other tokens a bit more interesting.  \n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #when exponenting -inf for softmax, we want it to output 0 and for 0 we want it to output 1 // we are saying in this layer basically is past tokens cannot talk to each other.\n",
    "wei = wei.softmax(1)\n",
    "print(wei)\n",
    "xbow2 = wei @ x\n",
    "torch.allclose(xbow2, xbow) # it is true for all of the values x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 8, 8])\n",
      "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
      "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
      "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
      "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
      "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
      "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
      "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
      "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
      "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
      "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
      "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
      "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
      "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
      "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
      "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
      "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "# 2, 4 4,3\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) \n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "#what we are doing is single head attention, we create bunch of 'questions(headsize) and keys(headsize)' with key query matrix and dot product those questions with our\n",
    "#initial matrix which is x. once we get those matrixes, we dot product key and query\n",
    "# if dot product is higher, the affect of change is much higher.\n",
    "\n",
    "k = key(x) #dot product with key matrix (B, T, headsize)\n",
    "q = query(x) #dot product with query matrix (B, T, headsize)\n",
    "\n",
    "#i think the great way to think about these dot products above is that we have bunch of questions and that question number is head size and\n",
    "#and there are bunch of people that will respond to our question (T number), now we ask our one question to every people and our second question to every people and so on until everyone is being questioned about every question we have \n",
    "#and the amount of words, amount of effort that we are going to spend for people to answer that questions is our C channel.\n",
    "#if we increase C channel, we can get much more detailed answers about our asked questions. we can also increase amount of questions that we will ask and so on to reach our goal of understanding context of our questions for every people.\n",
    "\n",
    "#cannot do dot product with (B, T, headsize) (B, T, headsize) so converting key to (B, headsize, T)\n",
    "wei = q @ k.transpose(-2, -1) #(B,T,T) gives us how much results from the query matrix align with the key matrix.\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "# wei = torch.zeros((T,T)) #this will comment out since we want data dependable wei.\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # (B,T,T)\n",
    "wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "print(wei[0])\n",
    "print(wei.shape)\n",
    "\n",
    "v = value(x) # (B,T, headsize) #this value matrix stores kinda private information of the given input, you can think as key query matrix resolves and gives you nice dot product value\n",
    "#but even though query and key pairs are communicated ok, value pairs determine how are those questions alligment matter.\n",
    "#the resulting aligment of query key matrix may be ok or not ok for some results.\n",
    "\n",
    "\n",
    "out = wei @ v # (B,T, headsize)\n",
    "#1. another way to think about attention is that every token, in our case char, has a information vector about itself and this vector aggregate information via weighted sum of every other\n",
    "#node that points to it. for ex: first node only points to itself, second node points to first node and itself.third points to first second and itself and so on.\n",
    "#last node which is determined by block size is weighted sum vector of all of the nodes pointing to it and itself.\n",
    "#2. attention can be applied to any arbitary directed graph and is just a communication principle between graph \n",
    "\n",
    "#3. these nodes have no understanding of the space they are currently belong which other terms mean they dont know where they are in context. so we should encode them\n",
    "#with their position. \n",
    " \n",
    "#4. important thing to notice is that batches of examples never talk to each other. we do single head attention logic on single batch and not talk to any other batch.\n",
    "#by batch i mean the exact number of #(B,T,T) where batch number is B\n",
    "\n",
    "#5. by apllying masking, we created 'decoder' block where it kinda decodes the text and its kinda context and everythinh gradually and not looking to future tokens\n",
    "#but when making such as sentiment analysis, you can just delete masking line and you will have fully talking with each other tokens.\n",
    "\n",
    "#6. we have 'self-attention' block which means keys, querys, and values are generated from dot product of x, like they are generated from the example we have given, generated from batch\n",
    "#while 'cross-attention' gets keys, values but not querys from other source of matrix, external source of matrix mult of some nodes.  \n",
    "#these matrixes could be come from some encoder blocks that we want to convey some specific context, some specific meaning to tokens and test them over self-generated queries.\n",
    "#in cross attention we are just producing queries within ourselves but reading information externally\n",
    "#basically wwe use this if there is seperate source of nodes we want to pool information from to our nodes \n",
    "\n",
    "#7 we miss one variable from paper softmax( Q @ T / âˆšdk) @ V. dk is headsize. wei when taken its softmax converges to extreme values at initilazation if there is big values present in the vector\n",
    "#and basically becomes one hot vector, it means that you are aggregating information from the result of attention block from just one node.\n",
    "#this is not we want especially in init. so dk used for normalizing these values before softmax.\n",
    "\n",
    "\n",
    "\n",
    "#this attention mechanism can be applied to any \n",
    "\n",
    "\n",
    "print(out[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1, 0, 0],\n",
      "        [0, 2, 0, 2]])\n",
      "tensor([[2, 2, 1],\n",
      "        [1, 2, 1],\n",
      "        [0, 2, 2],\n",
      "        [0, 0, 0]])\n",
      "tensor([[5, 6, 3],\n",
      "        [2, 4, 2]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0,3, (2,4))\n",
    "b = torch.randint(0,3, (4,3))\n",
    "print(a)\n",
    "print(b)\n",
    "c = a @ b \n",
    "print(c)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
